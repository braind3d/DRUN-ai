{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRUN Deep Q-learning driving network (regular observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Deep Q-Learning network which uses the Microsoft AirSim simulation wrapped in OpenAI gym enviourment class for training, practising navigation from point A on a map to point B without colliding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Open AI gym\n",
    "Installing our custom \"airsim_gym\" gym enviourment package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -e airsim_gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other libraries\n",
    "Importing all the libraries used in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from collections import namedtuple, deque\n",
    "from math import exp\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, Activation, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import Huber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"airsim_gym:airsim-regular-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "STATE_SIZE = [256, 256, 4]\n",
    "ACTION_SIZE = env.action_space.n\n",
    "STACK_SIZE = 64\n",
    "LEARNING_RATE = 0.0002\n",
    "\n",
    "# Training parameters\n",
    "TOTAL_EPISODES = 5000\n",
    "MAX_STEPS = 1000\n",
    "BATCH_SIZE = 64\n",
    "PRETRAIN_LENGTH = BATCH_SIZE\n",
    "MEMORY_SIZE = 1000000\n",
    "UPDATE_AFTER_ACTIONS = 4\n",
    "\n",
    "# Epsilon greedy\n",
    "EXPLORE_START = 1.0\n",
    "EXPLORE_STOP = 0.01\n",
    "DECAY_RATE = 0.0001\n",
    "\n",
    "# Q-learning hyperparameters\n",
    "GAMMA = 0.95\n",
    "\n",
    "# Script execution\n",
    "TRAINING = True\n",
    "ENV_PREVIEW = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENV_PREVIEW:\n",
    "    env.reset()\n",
    "    for _ in range(10):\n",
    "        env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image processing utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepocess_frame\n",
    "Preprocessing in order to reduce the complexity of our states and consecutively to reduce the computation time needed for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    # Converts frame from RGB to grayscale\n",
    "    grayscale_frame = np.mean(frame, -1)\n",
    "\n",
    "    # Normalize Pixel Values\n",
    "    normalized_frame = grayscale_frame/255.0\n",
    "\n",
    "    return normalized_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stack_frames\n",
    "Stacking frames in or to crate a sense of motion to our Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_frames(stacked_frames, state, is_new_episode: bool, stack_size: int = STACK_SIZE):\n",
    "    # Preprocess frame\n",
    "    frame = preprocess_frame(state)\n",
    "\n",
    "    if is_new_episode:\n",
    "\n",
    "        # Clear our stacked_frames\n",
    "        stacked_frames = [np.zeros(STATE_SIZE[:2], dtype=np.int) for i in range(stack_size)]\n",
    "        stacked_frames = deque(stacked_frames, maxlen=stack_size)\n",
    "\n",
    "        # In a new episode the deque is filled with the same frame\n",
    "        for _ in range(stack_size):\n",
    "            stacked_frames.append(frame)\n",
    "\n",
    "    else:\n",
    "        # Append frame to deque, pops the last\n",
    "        stacked_frames.append(frame)\n",
    "\n",
    "    # Build the stacked state (first dimension specifies different frames)\n",
    "    stacked_state = np.stack(stacked_frames, axis=2)\n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_frames = deque([np.zeros(STATE_SIZE[:2], dtype=np.int) for i in range(STACK_SIZE)], maxlen=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay memory\n",
    "Create the Memory object that contains a deque. A deque (double ended queue) is a data type that removes the oldest element each time that you add a new element over the size limit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expiriance replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple(\n",
    "    \"Experience\",\n",
    "    (\"observation\", \"position\", \"action\", \"next_observation\", \"next_position\", \"reward\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define replay memory class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.push_count = 0\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        self.push_count += 1\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        buffer_size = len(self.buffer)\n",
    "        index = np.random.choice(\n",
    "            np.arange(buffer_size),\n",
    "            size=batch_size,\n",
    "            replace=False\n",
    "        )\n",
    "\n",
    "        return [self.buffer[i] for i in index]\n",
    "\n",
    "    def is_sample_available(self, batch_size):\n",
    "        return len(self.buffer) >= batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_memory = ReplayMemory(MEMORY_SIZE)\n",
    "observation_stack = None\n",
    "next_observation_stack = None\n",
    "next_action = None\n",
    "done = False\n",
    "observation = None\n",
    "\n",
    "for i in range(PRETRAIN_LENGTH):\n",
    "    if i == 0:\n",
    "        # If no state is available, we get one from the reset\n",
    "        start_observation, position = env.reset()\n",
    "        _, observation_stack = stack_frames(\n",
    "            observation_stack,\n",
    "            start_observation,\n",
    "            True,\n",
    "        )\n",
    "        _, next_observation_stack = stack_frames(\n",
    "            next_observation_stack,\n",
    "            start_observation,\n",
    "            True,\n",
    "        )\n",
    "\n",
    "    # Random action\n",
    "    if (observation is None):\n",
    "        action = env.action_space.sample()\n",
    "        observation, position, reward, done = env.step(action)\n",
    "\n",
    "        _, next_observation_stack = stack_frames(\n",
    "            next_observation_stack,\n",
    "            observation,\n",
    "            False,\n",
    "        )\n",
    "\n",
    "    _, observation_stack = stack_frames(\n",
    "        observation_stack,\n",
    "        observation,\n",
    "        False,\n",
    "    )\n",
    "\n",
    "    # Hit something\n",
    "    if done:\n",
    "        print(\"done\")\n",
    "        # Empty frame on episode ending\n",
    "        next_observation = np.zeros(STATE_SIZE[:2], dtype=np.float32)\n",
    "        _, next_observation_stack = stack_frames(\n",
    "            next_observation_stack,\n",
    "            next_observation,\n",
    "            False,\n",
    "        )\n",
    "        next_position = position\n",
    "\n",
    "        # Add experience to memory\n",
    "        replay_memory.add(\n",
    "            Experience(\n",
    "                observation_stack,\n",
    "                position,\n",
    "                action,\n",
    "                next_observation_stack,\n",
    "                next_position,\n",
    "                reward,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Start a new episode\n",
    "        start_observation, position = env.reset()\n",
    "        _, observation_stack = stack_frames(\n",
    "            observation_stack,\n",
    "            start_observation,\n",
    "            True,\n",
    "        )\n",
    "        _, next_observation_stack = stack_frames(\n",
    "            next_observation_stack,\n",
    "            start_observation,\n",
    "            True,\n",
    "        )\n",
    "\n",
    "        observation = None\n",
    "        position = None\n",
    "        done = False\n",
    "    else:\n",
    "        # Get the next state\n",
    "        next_observation, next_position, next_action, next_done = env.step(action)\n",
    "        _, next_observation_stack = stack_frames(\n",
    "            next_observation_stack,\n",
    "            observation,\n",
    "            False,\n",
    "        )\n",
    "\n",
    "        # Add experience to memory\n",
    "        replay_memory.add(\n",
    "            Experience(\n",
    "                observation_stack,\n",
    "                position,\n",
    "                action,\n",
    "                next_observation_stack,\n",
    "                next_position,\n",
    "                reward,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Our state is now the next_observation\n",
    "        observation = next_observation\n",
    "        position = next_position\n",
    "        done = next_done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon greedy strategy\n",
    "$\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s_t,a)$. Over time the exploration probability decays in favour of the exploatation rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy():\n",
    "    def __init__(self, start, stop, decay):\n",
    "        self.start = start\n",
    "        self.stop = stop\n",
    "        self.decay = decay\n",
    "\n",
    "    def get_exploration_rate(self, current_step):\n",
    "        rate = self.stop + (self.start - self.stop)\n",
    "        rate *= exp(-1 * current_step * self.decay)\n",
    "        return rate\n",
    "\n",
    "    def predict_action(self, current_step, observation, position, env, dqn):\n",
    "        # Randomizing a number\n",
    "        exp_exp_tradeoff = np.random.rand()\n",
    "\n",
    "        explore_probability = self.get_exploration_rate(current_step)\n",
    "\n",
    "        if explore_probability < exp_exp_tradeoff:\n",
    "            # A random action is sampled\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        else:\n",
    "            # Get action from Q-network (exploitation)\n",
    "            # Estimate the Qs values state\n",
    "            observation = np.array(observation)\n",
    "            position = np.array(position)\n",
    "\n",
    "            observation = observation.reshape(1, *observation.shape)\n",
    "            position = position.reshape(1, *position.shape)\n",
    "\n",
    "            print(observation.shape, position.shape)\n",
    "            prediction = dqn.predict([observation, position])\n",
    "\n",
    "            # Take the biggest Q value (= the best action)\n",
    "            action = np.argmax(prediction)\n",
    "\n",
    "        return action, explore_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = EpsilonGreedy(EXPLORE_START, EXPLORE_START, DECAY_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q-learning network\n",
    "This is our Deep Q-learning model:\n",
    "\n",
    "We take a stack of 4 frames and two normalized coordinates as input:\n",
    "- Image is passed through 3 CNN layers\n",
    "- Then it is concatinated with the coordinates\n",
    "- Finally it passes through 3 FC layers\n",
    "- Outputs a Q value for each actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drun_dqn() -> Model:\n",
    "    image_input = Input(STATE_SIZE)\n",
    "    coords_input = Input(2)\n",
    "\n",
    "    img_net = Conv2D(32, (4, 4), strides=(4, 4), activation=\"relu\", padding=\"same\", input_shape=STATE_SIZE)(image_input)\n",
    "    img_net = Conv2D(64, (3, 3), strides=(2, 2), activation=\"relu\", padding=\"same\")(img_net)\n",
    "    img_net = Conv2D(64, (3, 3), strides=(2, 2), activation=\"relu\", padding=\"same\")(img_net)\n",
    "    img_net = Flatten()(img_net)\n",
    "\n",
    "    combined = Concatenate(axis=1)\n",
    "    combined = combined([img_net, coords_input])\n",
    "\n",
    "    dense_net = Dense(512, activation=tf.nn.relu)(combined)\n",
    "    dense_net = Dense(512, activation=tf.nn.relu)(dense_net)\n",
    "    dense_net = Dense(512, activation=tf.nn.relu)(dense_net)\n",
    "    output = Dense(ACTION_SIZE, activation=tf.nn.elu)(dense_net)\n",
    "\n",
    "    return Model(inputs=(image_input, coords_input), outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = drun_dqn()\n",
    "optimizer = Adam(learning_rate=LEARNING_RATE, clipnorm=1.0)\n",
    "loss_function = Huber()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standart Q-learning algorithm:\n",
    "\n",
    "1. Initialize replay memory capacity.\n",
    "2. Initialize the policy network with random weights.\n",
    "3. Clone the policy network, and call it the target network.\n",
    "4. For each episode:\n",
    "    1. Initialize the starting state.\n",
    "    2. For each time step:\n",
    "        1. Select an action.\n",
    "            - Via exploration or exploitation\n",
    "        2. Execute selected action in an emulator.\n",
    "        3. Observe reward and next state.\n",
    "        4. Store experience in replay memory.\n",
    "        5. Sample random batch from replay memory.\n",
    "        6. Preprocess states from batch.\n",
    "        7. Pass batch of preprocessed states to policy network.\n",
    "        8. Calculate loss between output Q-values and target Q-values.\n",
    "            - Requires a pass to the target network for the next state\n",
    "        9. Gradient descent updates weights in the policy network to minimize loss.\n",
    "            - After  time steps, weights in the target network are updated to the weights in the policy network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAINING:\n",
    "    decay_step = 0\n",
    "\n",
    "    for episode in range(TOTAL_EPISODES):\n",
    "        episode_step = 0\n",
    "        episode_rewards = []\n",
    "\n",
    "        observation, position = env.reset()\n",
    "        observation, stacked_frames = stack_frames(stacked_frames, observation, True)\n",
    "\n",
    "        while episode_step < MAX_STEPS:\n",
    "            # Increase episode_decay/decay_steps\n",
    "            episode_step += 1\n",
    "            decay_step += 1\n",
    "\n",
    "            # Predict the action to take and take it\n",
    "            action, explore_probability = epsilon.predict_action(decay_step, observation, position, env, model)\n",
    "\n",
    "            # Do the action\n",
    "            observation, position, reward, done = env.step(action)\n",
    "            observation = preprocess_frame(observation)\n",
    "\n",
    "            # Add the reward to total reward\n",
    "            episode_rewards.append(reward)\n",
    "\n",
    "            # If the game is finished\n",
    "            if done:\n",
    "                # Empty frame on episode ending\n",
    "                next_observation = np.zeros(observation.shape)\n",
    "                next_position = [0.0, 0.0]\n",
    "\n",
    "                # Add experience to memory\n",
    "                replay_memory.add(Experience(stacked_frames, position, action, next_observation, next_position, reward, done))\n",
    "\n",
    "                # Start a new episode\n",
    "                observation, position = env.reset()\n",
    "\n",
    "                # Stack the frames\n",
    "                observation, stacked_frames = stack_frames(stacked_frames, observation, True)\n",
    "\n",
    "                # Set episode_step = max_steps to end the episode\n",
    "                episode_step = MAX_STEPS\n",
    "\n",
    "                # Get the total reward of the episode\n",
    "                total_reward = np.sum(episode_rewards)\n",
    "\n",
    "                print(\"Episode: {}\".format(episode),\n",
    "                      \"Total reward: {}\".format(total_reward),\n",
    "                      \"Explore probability: {:.4f}\".format(explore_probability))\n",
    "\n",
    "                replay_memory.add(Experience(stacked_frames, position, action, next_observation, next_position, reward, done))\n",
    "\n",
    "            else:\n",
    "                # Get the next state\n",
    "                next_observation, next_position = env.get_state()\n",
    "                next_observation, stacked_frames = stack_frames(stacked_frames, next_observation, False)\n",
    "\n",
    "                # Add experience to memory\n",
    "                replay_memory.add(Experience(stacked_frames, position, action, next_observation, next_position, reward, done))\n",
    "\n",
    "                # st+1 is now our current state\n",
    "                observation = next_observation\n",
    "\n",
    "            # LEARNING PART\n",
    "            # Obtain random mini-batch from memory\n",
    "            if episode_step % UPDATE_AFTER_ACTIONS == 0 and replay_memory.is_sample_available(BATCH_SIZE):\n",
    "                batch = replay_memory.sample(BATCH_SIZE)\n",
    "                observation_mb = np.array([item.observation for item in batch])\n",
    "                observation_mb = np.rollaxis(observation_mb, 1, observation_mb.ndim)\n",
    "                position_mb = np.array([item.position for item in batch])\n",
    "                actions_mb = np.array([item.action for item in batch])\n",
    "                next_observations_mb = np.array([item.next_observation for item in batch])\n",
    "                next_positions_mb = np.array([item.next_position for item in batch])\n",
    "                rewards_mb = np.array([item.reward for item in batch])\n",
    "                dones_mb = np.array([item.done for item in batch])\n",
    "\n",
    "                print(observation.shape, position_mb.shape)\n",
    "                target_Qs_batch = []\n",
    "\n",
    "                # Build the updated Q-values for the sampled future states\n",
    "                # Use the target model for stability\n",
    "    \n",
    "                future_rewards = model.predict([observation_mb, position_mb])\n",
    "                # Q value = reward + discount factor * expected future reward\n",
    "                updated_q_values = rewards_mb + GAMMA * tf.reduce_max(\n",
    "                    future_rewards, axis=1\n",
    "                )\n",
    "\n",
    "                # If final frame set the last value to -1\n",
    "                updated_q_values = updated_q_values * (1 - dones_mb) - dones_mb\n",
    "\n",
    "                # Create a mask so we only calculate loss on the updated Q-values\n",
    "                masks = tf.one_hot(actions_mb, ACTION_SIZE)\n",
    "                \n",
    "                # Train the model on the states and updated Q-values\n",
    "                q_values = model([observation_mb, position_mb])\n",
    "\n",
    "                # Apply the masks to the Q-values to get the Q-value for action taken\n",
    "                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "                # Calculate loss between new Q-value and old Q-value\n",
    "                loss = loss_function(updated_q_values, q_action)\n",
    "                print(\"Training loss: {:.4f}\".format(loss))\n",
    "\n",
    "                # Backpropagation\n",
    "                grads = tape.gradient(loss, model.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        # Save model every 10 episodes\n",
    "        if episode % 10 == 0:\n",
    "            model.save(\"model/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
